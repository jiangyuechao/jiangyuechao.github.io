<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="爬虫-Scrapy, 且听风吟">
    <meta name="description" content="welcome to my word">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>爬虫-Scrapy | 且听风吟</title>
    <link rel="icon" type="image/png" href="/source/images/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 6.2.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head>




<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">且听风吟</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a target="_blank" rel="noopener" href="http://jyc0507.top/#articles" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>主页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">且听风吟</div>
        <div class="logo-desc">
            
            welcome to my word
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a target="_blank" rel="noopener" href="http://jyc0507.top/#articles" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			主页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			分类
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/blinkfox/hexo-theme-matery" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/blinkfox/hexo-theme-matery" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/22.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">爬虫-Scrapy</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        padding: 35px 0 15px 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        padding-bottom: 30px;
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/%E7%88%AC%E8%99%AB-Scrapy/">
                                <span class="chip bg-color">爬虫-Scrapy</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/Scrapy/" class="post-category">
                                Scrapy
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2021-10-09
                </div>
                

                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.4k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    38 分
                </div>
                

                
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <span id="more"></span>






<p><strong>Scrapy</strong></p>
<h3 id="一-介绍"><a href="#一-介绍" class="headerlink" title="一 介绍"></a>一 介绍</h3><p>Scrapy一个开源和协作的框架，其最初是为了页面抓取 (更确切来说, 网络抓取 )所设计的，使用它可以以快速、简单、可扩展的方式从网站中提取所需的数据。但目前Scrapy的用途十分广泛，可用于如数据挖掘、监测和自动化测试等领域，也可以应用在获取API所返回的数据(例如 Amazon Associates Web Services ) 或者通用的网络爬虫。</p>
<p>Scrapy 是基于twisted框架开发而来，twisted是一个流行的事件驱动的python网络框架。因此Scrapy使用了一种非阻塞（又名异步）的代码来实现并发。</p>
<p><strong>The data flow in Scrapy is controlled by the execution engine, and goes like this:</strong></p>
<ol>
<li>The <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-engine">Engine</a> gets the initial Requests to crawl from the <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-spiders">Spider</a>.</li>
<li>The <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-engine">Engine</a> schedules the Requests in the <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-scheduler">Scheduler</a> and asks for the next Requests to crawl.</li>
<li>The <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-scheduler">Scheduler</a> returns the next Requests to the <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-engine">Engine</a>.</li>
<li>The <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-engine">Engine</a> sends the Requests to the <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-downloader">Downloader</a>, passing through the <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-downloader-middleware">Downloader Middlewares</a> (see <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"><code>process_request()</code></a>).</li>
<li>Once the page finishes downloading the <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-downloader">Downloader</a> generates a Response (with that page) and sends it to the Engine, passing through the <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-downloader-middleware">Downloader Middlewares</a> (see <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"><code>process_response()</code></a>).</li>
<li>The <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-engine">Engine</a> receives the Response from the <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-downloader">Downloader</a> and sends it to the <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-spiders">Spider</a> for processing, passing through the <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-spider-middleware">Spider Middleware</a> (see <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/spider-middleware.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input"><code>process_spider_input()</code></a>).</li>
<li>The <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-spiders">Spider</a> processes the Response and returns scraped items and new Requests (to follow) to the <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-engine">Engine</a>, passing through the <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-spider-middleware">Spider Middleware</a> (see <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/spider-middleware.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"><code>process_spider_output()</code></a>).</li>
<li>The <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-engine">Engine</a> sends processed items to <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-pipelines">Item Pipelines</a>, then send processed Requests to the <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-scheduler">Scheduler</a> and asks for possible next Requests to crawl.</li>
<li>The process repeats (from step 1) until there are no more requests from the <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-scheduler">Scheduler</a>.</li>
</ol>
<p><strong>Components：</strong></p>
<ol>
<li><p>引擎(EGINE)</p>
<p>引擎负责控制系统所有组件之间的数据流，并在某些动作发生时触发事件。有关详细信息，请参见上面的数据流部分。</p>
</li>
<li><p><strong>调度器(SCHEDULER)</strong><br>用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL的优先级队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址</p>
</li>
<li><p><strong>下载器(DOWLOADER)</strong><br>用于下载网页内容, 并将网页内容返回给EGINE，下载器是建立在twisted这个高效的异步模型上的</p>
</li>
<li><p><strong>爬虫(SPIDERS)</strong><br>SPIDERS是开发人员自定义的类，用来解析responses，并且提取items，或者发送新的请求</p>
</li>
<li><p><strong>项目管道(ITEM PIPLINES)</strong><br>在items被提取后负责处理它们，主要包括清理、验证、持久化（比如存到数据库）等操作</p>
</li>
<li><p>下载器中间件(Downloader Middlewares)</p>
<p>位于Scrapy引擎和下载器之间，主要用来处理从EGINE传到DOWLOADER的请求request，已经从DOWNLOADER传到EGINE的响应response，你可用该中间件做以下几件事</p>
<ol>
<li>process a request just before it is sent to the Downloader (i.e. right before Scrapy sends the request to the website);</li>
<li>change received response before passing it to a spider;</li>
<li>send a new Request instead of passing received response to a spider;</li>
<li>pass response to a spider without fetching a web page;</li>
<li>silently drop some requests.</li>
</ol>
</li>
<li><p><strong>爬虫中间件(Spider Middlewares)</strong><br>位于EGINE和SPIDERS之间，主要工作是处理SPIDERS的输入（即responses）和输出（即requests）</p>
</li>
</ol>
<p><a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html">官网链接：https://docs.scrapy.org/en/latest/topics/architecture.html</a></p>
<h3 id="二-安装"><a href="#二-安装" class="headerlink" title="二 安装"></a>二 安装</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Windows平台</span></span><br><span class="line">    <span class="number">1</span>、pip3 install wheel <span class="comment">#安装后，便支持通过wheel文件安装软件，wheel文件官网：https://www.lfd.uci.edu/~gohlke/pythonlibs</span></span><br><span class="line">    <span class="number">3</span>、pip3 install lxml</span><br><span class="line">    <span class="number">4</span>、pip3 install pyopenssl</span><br><span class="line">    <span class="number">5</span>、下载并安装pywin32：https://sourceforge.net/projects/pywin32/files/pywin32/</span><br><span class="line">    <span class="number">6</span>、下载twisted的wheel文件：http://www.lfd.uci.edu/~gohlke/pythonlibs/<span class="comment">#twisted</span></span><br><span class="line">    <span class="number">7</span>、执行pip3 install 下载目录\Twisted-<span class="number">17.9</span><span class="number">.0</span>-cp36-cp36m-win_amd64.whl</span><br><span class="line">    <span class="number">8</span>、pip3 install scrapy</span><br><span class="line"></span><br><span class="line"><span class="comment">#Linux平台</span></span><br><span class="line">    <span class="number">1</span>、pip3 install scrapy</span><br></pre></td></tr></tbody></table></figure>

<h3 id="三-命令行工具"><a href="#三-命令行工具" class="headerlink" title="三 命令行工具"></a>三 命令行工具</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1 查看帮助</span></span><br><span class="line">    scrapy -h</span><br><span class="line">    scrapy &lt;command&gt; -h</span><br><span class="line"></span><br><span class="line"><span class="comment">#2 有两种命令：其中Project-only必须切到项目文件夹下才能执行，而Global的命令则不需要</span></span><br><span class="line">    Global commands:</span><br><span class="line">        startproject <span class="comment">#创建项目</span></span><br><span class="line">        genspider    <span class="comment">#创建爬虫程序</span></span><br><span class="line">        settings     <span class="comment">#如果是在项目目录下，则得到的是该项目的配置</span></span><br><span class="line">        runspider    <span class="comment">#运行一个独立的python文件，不必创建项目</span></span><br><span class="line">        shell        <span class="comment">#scrapy shell url地址  在交互式调试，如选择器规则正确与否</span></span><br><span class="line">        fetch        <span class="comment">#独立于程单纯地爬取一个页面，可以拿到请求头</span></span><br><span class="line">        view         <span class="comment">#下载完毕后直接弹出浏览器，以此可以分辨出哪些数据是ajax请求</span></span><br><span class="line">        version      <span class="comment">#scrapy version 查看scrapy的版本，scrapy version -v查看scrapy依赖库的版本</span></span><br><span class="line">    Project-only commands:</span><br><span class="line">        crawl        <span class="comment">#运行爬虫，必须创建项目才行，确保配置文件中ROBOTSTXT_OBEY = False</span></span><br><span class="line">        check        <span class="comment">#检测项目中有无语法错误</span></span><br><span class="line">        <span class="built_in">list</span>         <span class="comment">#列出项目中所包含的爬虫名</span></span><br><span class="line">        edit         <span class="comment">#编辑器，一般不用</span></span><br><span class="line">        parse        <span class="comment">#scrapy parse url地址 --callback 回调函数  #以此可以验证我们的回调函数是否正确</span></span><br><span class="line">        bench        <span class="comment">#scrapy bentch压力测试</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#3 官网链接</span></span><br><span class="line">    https://docs.scrapy.org/en/latest/topics/commands.html</span><br><span class="line"><span class="comment">#1、执行全局命令：请确保不在某个项目的目录下，排除受该项目配置的影响</span></span><br><span class="line">scrapy startproject MyProject</span><br><span class="line"></span><br><span class="line">cd MyProject</span><br><span class="line">scrapy genspider baidu www.baidu.com</span><br><span class="line"></span><br><span class="line">scrapy settings --get XXX <span class="comment">#如果切换到项目目录下，看到的则是该项目的配置</span></span><br><span class="line"></span><br><span class="line">scrapy runspider baidu.py</span><br><span class="line"></span><br><span class="line">scrapy shell https://www.baidu.com</span><br><span class="line">    response</span><br><span class="line">    response.status</span><br><span class="line">    response.body</span><br><span class="line">    view(response)</span><br><span class="line">    </span><br><span class="line">scrapy view https://www.taobao.com <span class="comment">#如果页面显示内容不全，不全的内容则是ajax请求实现的，以此快速定位问题</span></span><br><span class="line"></span><br><span class="line">scrapy fetch --nolog --headers https://www.taobao.com</span><br><span class="line"></span><br><span class="line">scrapy version <span class="comment">#scrapy的版本</span></span><br><span class="line"></span><br><span class="line">scrapy version -v <span class="comment">#依赖库的版本</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2、执行项目命令：切到项目目录下</span></span><br><span class="line">scrapy crawl baidu</span><br><span class="line">scrapy check</span><br><span class="line">scrapy <span class="built_in">list</span></span><br><span class="line">scrapy parse http://quotes.toscrape.com/ --callback parse</span><br><span class="line">scrapy bench</span><br><span class="line">    </span><br></pre></td></tr></tbody></table></figure>

<h3 id="四-项目结构以及爬虫应用简介"><a href="#四-项目结构以及爬虫应用简介" class="headerlink" title="四 项目结构以及爬虫应用简介"></a>四 项目结构以及爬虫应用简介</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">project_name/</span><br><span class="line">   scrapy.cfg</span><br><span class="line">   project_name/</span><br><span class="line">       __init__.py</span><br><span class="line">       items.py</span><br><span class="line">       pipelines.py</span><br><span class="line">       settings.py</span><br><span class="line">       spiders/</span><br><span class="line">           __init__.py</span><br><span class="line">           爬虫<span class="number">1.</span>py</span><br><span class="line">           爬虫<span class="number">2.</span>py</span><br><span class="line">           爬虫<span class="number">3.</span>py</span><br></pre></td></tr></tbody></table></figure>

<p>文件说明：</p>
<ul>
<li>scrapy.cfg 项目的主配置信息，用来部署scrapy时使用，爬虫相关的配置信息在settings.py文件中。</li>
<li>items.py 设置数据存储模板，用于结构化数据，如：Django的Model</li>
<li>pipelines 数据处理行为，如：一般结构化的数据持久化</li>
<li>settings.py 配置文件，如：递归的层数、并发数，延迟下载等。*<em>强调:配置文件的选项必须大写否则视为无效*</em>*<em>，正确写法USER_AGENT=’xxxx’</em>*</li>
<li>spiders 爬虫目录，如：创建文件，编写爬虫规则</li>
</ul>
<p><em>注意：一般创建爬虫文件时，以网站域名命名</em></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在项目目录下新建：entrypoint.py</span></span><br><span class="line"><span class="keyword">from</span> scrapy.cmdline <span class="keyword">import</span> execute</span><br><span class="line">execute([<span class="string">'scrapy'</span>, <span class="string">'crawl'</span>, <span class="string">'xiaohua'</span>])</span><br><span class="line"><span class="keyword">import</span> sys,os</span><br><span class="line">sys.stdout=io.TextIOWrapper(sys.stdout.buffer,encoding=<span class="string">'gb18030'</span>)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="五-Spiders"><a href="#五-Spiders" class="headerlink" title="五 Spiders"></a>五 Spiders</h3><p><strong>1、介绍</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1、Spiders是由一系列类（定义了一个网址或一组网址将被爬取）组成，具体包括如何执行爬取任务并且如何从页面中提取结构化的数据。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2、换句话说，Spiders是你为了一个特定的网址或一组网址自定义爬取和解析页面行为的地方</span></span><br></pre></td></tr></tbody></table></figure>

<p><strong>2、Spiders会循环做如下事情</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1、生成初始的Requests来爬取第一个URLS，并且标识一个回调函数</span></span><br><span class="line">第一个请求定义在start_requests()方法内默认从start_urls列表中获得url地址来生成Request请求，默认的回调函数是parse方法。回调函数在下载完成返回response时自动触发</span><br><span class="line"></span><br><span class="line"><span class="comment">#2、在回调函数中，解析response并且返回值</span></span><br><span class="line">返回值可以<span class="number">4</span>种：</span><br><span class="line">        包含解析数据的字典</span><br><span class="line">        Item对象</span><br><span class="line">        新的Request对象（新的Requests也需要指定一个回调函数）</span><br><span class="line">        或者是可迭代对象（包含Items或Request）</span><br><span class="line"></span><br><span class="line"><span class="comment">#3、在回调函数中解析页面内容</span></span><br><span class="line">通常使用Scrapy自带的Selectors，但很明显你也可以使用Beutifulsoup，lxml或其他你爱用啥用啥。</span><br><span class="line"></span><br><span class="line"><span class="comment">#4、最后，针对返回的Items对象将会被持久化到数据库</span></span><br><span class="line">通过Item Pipeline组件存到数据库：https://docs.scrapy.org/en/latest/topics/item-pipeline.html<span class="comment">#topics-item-pipeline）</span></span><br><span class="line">或者导出到不同的文件（通过Feed exports：https://docs.scrapy.org/en/latest/topics/feed-exports.html<span class="comment">#topics-feed-exports）</span></span><br></pre></td></tr></tbody></table></figure>

<p><strong>3、Spiders总共提供了五种类：</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1、scrapy.spiders.Spider #scrapy.Spider等同于scrapy.spiders.Spider</span></span><br><span class="line"><span class="comment">#2、scrapy.spiders.CrawlSpider</span></span><br><span class="line"><span class="comment">#3、scrapy.spiders.XMLFeedSpider</span></span><br><span class="line"><span class="comment">#4、scrapy.spiders.CSVFeedSpider</span></span><br><span class="line"><span class="comment">#5、scrapy.spiders.SitemapSpider</span></span><br></pre></td></tr></tbody></table></figure>

<p><strong>4、导入使用</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> Spider,CrawlSpider,XMLFeedSpider,CSVFeedSpider,SitemapSpider</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AmazonSpider</span>(scrapy.Spider): <span class="comment">#自定义类，继承Spiders提供的基类</span></span><br><span class="line">    name = <span class="string">'amazon'</span></span><br><span class="line">    allowed_domains = [<span class="string">'www.amazon.cn'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://www.amazon.cn/'</span>]</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></tbody></table></figure>

<p><strong>5、class scrapy.spiders.Spider</strong></p>
<p>这是最简单的spider类，任何其他的spider类都需要继承它（包含你自己定义的）。</p>
<p>该类不提供任何特殊的功能，它仅提供了一个默认的start_requests方法默认从start_urls中读取url地址发送requests请求，并且默认parse作为回调函数</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AmazonSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">'amazon'</span> </span><br><span class="line"></span><br><span class="line">```</span><br><span class="line">allowed_domains = [<span class="string">'www.amazon.cn'</span>] </span><br><span class="line"></span><br><span class="line">start_urls = [<span class="string">'http://www.amazon.cn/'</span>]</span><br><span class="line"></span><br><span class="line">custom_settings = {</span><br><span class="line">    <span class="string">'BOT_NAME'</span> : <span class="string">'Egon_Spider_Amazon'</span>,</span><br><span class="line">    <span class="string">'REQUEST_HEADERS'</span> : {</span><br><span class="line">      <span class="string">'Accept'</span>: <span class="string">'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'</span>,</span><br><span class="line">      <span class="string">'Accept-Language'</span>: <span class="string">'en'</span>,</span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"><span class="comment">#1、name = 'amazon' </span></span><br><span class="line">定义爬虫名，scrapy会根据该值定位爬虫程序</span><br><span class="line">所以它必须要有且必须唯一（In Python <span class="number">2</span> this must be ASCII only.）</span><br><span class="line"></span><br><span class="line"><span class="comment">#2、allowed_domains = ['www.amazon.cn'] </span></span><br><span class="line">定义允许爬取的域名，如果OffsiteMiddleware启动（默认就启动），</span><br><span class="line">那么不属于该列表的域名及其子域名都不允许爬取</span><br><span class="line">如果爬取的网址为：https://www.example.com/<span class="number">1.</span>html，那就添加<span class="string">'example.com'</span>到列表.</span><br><span class="line"></span><br><span class="line"><span class="comment">#3、start_urls = ['http://www.amazon.cn/']</span></span><br><span class="line">如果没有指定url，就从该列表中读取url来生成第一个请求</span><br><span class="line"></span><br><span class="line"><span class="comment">#4、custom_settings</span></span><br><span class="line">值为一个字典，定义一些配置信息，在运行爬虫程序时，这些配置会覆盖项目级别的配置</span><br><span class="line">所以custom_settings必须被定义成一个类属性，由于settings会在类实例化前被加载</span><br><span class="line"></span><br><span class="line"><span class="comment">#5、settings</span></span><br><span class="line">通过self.settings[<span class="string">'配置项的名字'</span>]可以访问settings.py中的配置，如果自己定义了custom_settings还是以自己的为准</span><br><span class="line"></span><br><span class="line"><span class="comment">#6、logger</span></span><br><span class="line">日志名默认为spider的名字</span><br><span class="line">self.logger.debug(<span class="string">'=============&gt;%s'</span> %self.settings[<span class="string">'BOT_NAME'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#5、crawler：了解</span></span><br><span class="line">该属性必须被定义到类方法from_crawler中</span><br><span class="line"></span><br><span class="line"><span class="comment">#6、from_crawler(crawler, *args, **kwargs)：了解</span></span><br><span class="line">You probably won’t need to override this directly  because the default implementation acts <span class="keyword">as</span> a proxy to the __init__() method, calling it <span class="keyword">with</span> the given arguments args <span class="keyword">and</span> named arguments kwargs.</span><br><span class="line"></span><br><span class="line"><span class="comment">#7、start_requests()</span></span><br><span class="line">该方法用来发起第一个Requests请求，且必须返回一个可迭代的对象。它在爬虫程序打开时就被Scrapy调用，Scrapy只调用它一次。</span><br><span class="line">默认从start_urls里取出每个url来生成Request(url, dont_filter=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#针对参数dont_filter,请看自定义去重规则</span></span><br><span class="line"></span><br><span class="line">如果你想要改变起始爬取的Requests，你就需要覆盖这个方法，例如你想要起始发送一个POST请求，如下</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MySpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">'myspider'</span></span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">start_requests</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="keyword">return</span> [scrapy.FormRequest(<span class="string">"http://www.example.com/login"</span>,</span><br><span class="line">                               formdata={<span class="string">'user'</span>: <span class="string">'john'</span>, <span class="string">'pass'</span>: <span class="string">'secret'</span>},</span><br><span class="line">                               callback=self.logged_in)]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">logged_in</span>(<span class="params">self, response</span>):</span><br><span class="line">    <span class="comment"># here you would extract links to follow and return Requests for</span></span><br><span class="line">    <span class="comment"># each of them, with another callback</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">```</span><br><span class="line"></span><br><span class="line"><span class="comment">#8、parse(response)</span></span><br><span class="line">这是默认的回调函数，所有的回调函数必须返回an iterable of Request <span class="keyword">and</span>/<span class="keyword">or</span> dicts <span class="keyword">or</span> Item objects.</span><br><span class="line"></span><br><span class="line"><span class="comment">#9、log(message[, level, component])：了解</span></span><br><span class="line">Wrapper that sends a log message through the Spider’s logger, kept <span class="keyword">for</span> backwards compatibility. For more information see Logging <span class="keyword">from</span> Spiders.</span><br><span class="line"></span><br><span class="line"><span class="comment">#10、closed(reason)</span></span><br><span class="line">爬虫程序结束时自动触发</span><br><span class="line">去重规则应该多个爬虫共享的，但凡一个爬虫爬取了，其他都不要爬了，实现方式如下</span><br><span class="line"></span><br><span class="line"><span class="comment">#方法一：</span></span><br><span class="line"><span class="number">1</span>、新增类属性</span><br><span class="line">visited=<span class="built_in">set</span>() <span class="comment">#类属性</span></span><br><span class="line"></span><br><span class="line"><span class="number">2</span>、回调函数parse方法内：</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">    <span class="keyword">if</span> response.url <span class="keyword">in</span> self.visited:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    .......</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line">self.visited.add(response.url) </span><br><span class="line">```</span><br><span class="line"></span><br><span class="line"><span class="comment">#方法一改进：针对url可能过长，所以我们存放url的hash值</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        url=md5(response.request.url)</span><br><span class="line">    <span class="keyword">if</span> url <span class="keyword">in</span> self.visited:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    .......</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line">self.visited.add(url) </span><br><span class="line">```</span><br><span class="line"></span><br><span class="line"><span class="comment">#方法二：Scrapy自带去重功能</span></span><br><span class="line">配置文件：</span><br><span class="line">DUPEFILTER_CLASS = <span class="string">'scrapy.dupefilter.RFPDupeFilter'</span> <span class="comment">#默认的去重规则帮我们去重，去重规则在内存中</span></span><br><span class="line">DUPEFILTER_DEBUG = <span class="literal">False</span></span><br><span class="line">JOBDIR = <span class="string">"保存范文记录的日志路径，如：/root/"</span>  <span class="comment"># 最终路径为 /root/requests.seen，去重规则放文件中</span></span><br><span class="line"></span><br><span class="line">scrapy自带去重规则默认为RFPDupeFilter，只需要我们指定</span><br><span class="line">Request(...,dont_filter=<span class="literal">False</span>) ，如果dont_filter=<span class="literal">True</span>则告诉Scrapy这个URL不参与去重。</span><br><span class="line"></span><br><span class="line"><span class="comment">#方法三：</span></span><br><span class="line">我们也可以仿照RFPDupeFilter自定义去重规则，</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy.dupefilter <span class="keyword">import</span> RFPDupeFilter，看源码，仿照BaseDupeFilter</span><br><span class="line"></span><br><span class="line"><span class="comment">#步骤一：在项目目录下自定义去重文件dup.py</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">UrlFilter</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.visited = <span class="built_in">set</span>() <span class="comment">#或者放到数据库</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_settings</span>(<span class="params">cls, settings</span>):</span><br><span class="line">        <span class="keyword">return</span> cls()</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">request_seen</span>(<span class="params">self, request</span>):</span><br><span class="line">    <span class="keyword">if</span> request.url <span class="keyword">in</span> self.visited:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    self.visited.add(request.url)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">open</span>(<span class="params">self</span>):  <span class="comment"># can return deferred</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">close</span>(<span class="params">self, reason</span>):  <span class="comment"># can return a deferred</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">log</span>(<span class="params">self, request, spider</span>):  <span class="comment"># log that a request has been filtered</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">```</span><br><span class="line"></span><br><span class="line"><span class="comment">#步骤二：配置文件settings.py：</span></span><br><span class="line">DUPEFILTER_CLASS = <span class="string">'项目名.dup.UrlFilter'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 源码分析：</span></span><br><span class="line"><span class="keyword">from</span> scrapy.core.scheduler <span class="keyword">import</span> Scheduler</span><br><span class="line">见Scheduler下的enqueue_request方法：self.df.request_seen(request)</span><br><span class="line"><span class="comment">#例一：</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MySpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">'example.com'</span></span><br><span class="line">    allowed_domains = [<span class="string">'example.com'</span>]</span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">'http://www.example.com/1.html'</span>,</span><br><span class="line">        <span class="string">'http://www.example.com/2.html'</span>,</span><br><span class="line">        <span class="string">'http://www.example.com/3.html'</span>,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">    self.logger.info(<span class="string">'A response from %s just arrived!'</span>, response.url)</span><br><span class="line">```</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="comment">#例二：一个回调函数返回多个Requests和Items</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MySpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">'example.com'</span></span><br><span class="line">    allowed_domains = [<span class="string">'example.com'</span>]</span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">'http://www.example.com/1.html'</span>,</span><br><span class="line">        <span class="string">'http://www.example.com/2.html'</span>,</span><br><span class="line">        <span class="string">'http://www.example.com/3.html'</span>,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">    <span class="keyword">for</span> h3 <span class="keyword">in</span> response.xpath(<span class="string">'//h3'</span>).extract():</span><br><span class="line">        <span class="keyword">yield</span> {<span class="string">"title"</span>: h3}</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> response.xpath(<span class="string">'//a/@href'</span>).extract():</span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(url, callback=self.parse)</span><br><span class="line">```</span><br><span class="line"></span><br><span class="line">            </span><br><span class="line"><span class="comment">#例三：在start_requests()内直接指定起始爬取的urls，start_urls就没有用了，</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> myproject.items <span class="keyword">import</span> MyItem</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MySpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">'example.com'</span></span><br><span class="line">    allowed_domains = [<span class="string">'example.com'</span>]</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">start_requests</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="keyword">yield</span> scrapy.Request(<span class="string">'http://www.example.com/1.html'</span>, self.parse)</span><br><span class="line">    <span class="keyword">yield</span> scrapy.Request(<span class="string">'http://www.example.com/2.html'</span>, self.parse)</span><br><span class="line">    <span class="keyword">yield</span> scrapy.Request(<span class="string">'http://www.example.com/3.html'</span>, self.parse)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">    <span class="keyword">for</span> h3 <span class="keyword">in</span> response.xpath(<span class="string">'//h3'</span>).extract():</span><br><span class="line">        <span class="keyword">yield</span> MyItem(title=h3)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> url <span class="keyword">in</span> response.xpath(<span class="string">'//a/@href'</span>).extract():</span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(url, callback=self.parse)</span><br><span class="line">我们可能需要在命令行为爬虫程序传递参数，比如传递初始的url，像这样</span><br><span class="line"><span class="comment">#命令行执行</span></span><br><span class="line">scrapy crawl myspider -a category=electronics</span><br><span class="line"></span><br><span class="line"><span class="comment">#在__init__方法中可以接收外部传进来的参数</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MySpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">'myspider'</span></span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, category=<span class="literal">None</span>, *args, **kwargs</span>):</span><br><span class="line">    <span class="built_in">super</span>(MySpider, self).__init__(*args, **kwargs)</span><br><span class="line">    self.start_urls = [<span class="string">'http://www.example.com/categories/%s'</span> % category]</span><br><span class="line">    <span class="comment">#...</span></span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"><span class="comment">#注意接收的参数全都是字符串，如果想要结构化的数据，你需要用类似json.loads的方法</span></span><br></pre></td></tr></tbody></table></figure>

<p><strong>6、<a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/spiders.html#generic-spiders">其他通用Spiders：https://docs.scrapy.org/en/latest/topics/spiders.html#generic-spiders</a></strong></p>
<h3 id="六-Selectors"><a href="#六-Selectors" class="headerlink" title="六 Selectors"></a>六 Selectors</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1 //与/</span></span><br><span class="line"><span class="comment">#2 text</span></span><br><span class="line"><span class="comment">#3、extract与extract_first:从selector对象中解出内容</span></span><br><span class="line"><span class="comment">#4、属性：xpath的属性加前缀@</span></span><br><span class="line"><span class="comment">#4、嵌套查找</span></span><br><span class="line"><span class="comment">#5、设置默认值</span></span><br><span class="line"><span class="comment">#4、按照属性查找</span></span><br><span class="line"><span class="comment">#5、按照属性模糊查找</span></span><br><span class="line"><span class="comment">#6、正则表达式</span></span><br><span class="line"><span class="comment">#7、xpath相对路径</span></span><br><span class="line"><span class="comment">#8、带变量的xpath</span></span><br><span class="line">response.selector.css()</span><br><span class="line">response.selector.xpath()</span><br><span class="line">可简写为</span><br><span class="line">response.css()</span><br><span class="line">response.xpath()</span><br><span class="line"></span><br><span class="line"><span class="comment">#1 //与/</span></span><br><span class="line">response.xpath(<span class="string">'//body/a/'</span>)<span class="comment">#</span></span><br><span class="line">response.css(<span class="string">'div a::text'</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.xpath(<span class="string">'//body/a'</span>) <span class="comment">#开头的//代表从整篇文档中寻找,body之后的/代表body的儿子</span></span><br><span class="line">[]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.xpath(<span class="string">'//body//a'</span>) <span class="comment">#开头的//代表从整篇文档中寻找,body之后的//代表body的子子孙孙</span></span><br><span class="line">[&lt;Selector xpath=<span class="string">'//body//a'</span> data=<span class="string">'&lt;a href="image1.html"&gt;Name: My image 1 &lt;'</span>&gt;, &lt;Selector xpath=<span class="string">'//body//a'</span> data=<span class="string">'&lt;a href="image2.html"&gt;Name: My image 2 &lt;'</span>&gt;, &lt;Selector xpath=<span class="string">'//body//a'</span> data=<span class="string">'&lt;a href="</span></span><br><span class="line"><span class="string">image3.html"&gt;Name: My image 3 &lt;'</span>&gt;, &lt;Selector xpath=<span class="string">'//body//a'</span> data=<span class="string">'&lt;a href="image4.html"&gt;Name: My image 4 &lt;'</span>&gt;, &lt;Selector xpath=<span class="string">'//body//a'</span> data=<span class="string">'&lt;a href="image5.html"&gt;Name: My image 5 &lt;'</span>&gt;]</span><br><span class="line"></span><br><span class="line"><span class="comment">#2 text</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.xpath(<span class="string">'//body//a/text()'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.css(<span class="string">'body a::text'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#3、extract与extract_first:从selector对象中解出内容</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.xpath(<span class="string">'//div/a/text()'</span>).extract()</span><br><span class="line">[<span class="string">'Name: My image 1 '</span>, <span class="string">'Name: My image 2 '</span>, <span class="string">'Name: My image 3 '</span>, <span class="string">'Name: My image 4 '</span>, <span class="string">'Name: My image 5 '</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.css(<span class="string">'div a::text'</span>).extract()</span><br><span class="line">[<span class="string">'Name: My image 1 '</span>, <span class="string">'Name: My image 2 '</span>, <span class="string">'Name: My image 3 '</span>, <span class="string">'Name: My image 4 '</span>, <span class="string">'Name: My image 5 '</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.xpath(<span class="string">'//div/a/text()'</span>).extract_first()</span><br><span class="line"><span class="string">'Name: My image 1 '</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.css(<span class="string">'div a::text'</span>).extract_first()</span><br><span class="line"><span class="string">'Name: My image 1 '</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#4、属性：xpath的属性加前缀@</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.xpath(<span class="string">'//div/a/@href'</span>).extract_first()</span><br><span class="line"><span class="string">'image1.html'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.css(<span class="string">'div a::attr(href)'</span>).extract_first()</span><br><span class="line"><span class="string">'image1.html'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#4、嵌套查找</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.xpath(<span class="string">'//div'</span>).css(<span class="string">'a'</span>).xpath(<span class="string">'@href'</span>).extract_first()</span><br><span class="line"><span class="string">'image1.html'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#5、设置默认值</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.xpath(<span class="string">'//div[@id="xxx"]'</span>).extract_first(default=<span class="string">"not found"</span>)</span><br><span class="line"><span class="string">'not found'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#4、按照属性查找</span></span><br><span class="line">response.xpath(<span class="string">'//div[@id="images"]/a[@href="image3.html"]/text()'</span>).extract()</span><br><span class="line">response.css(<span class="string">'#images a[@href="image3.html"]/text()'</span>).extract()</span><br><span class="line"></span><br><span class="line"><span class="comment">#5、按照属性模糊查找</span></span><br><span class="line">response.xpath(<span class="string">'//a[contains(@href,"image")]/@href'</span>).extract()</span><br><span class="line">response.css(<span class="string">'a[href*="image"]::attr(href)'</span>).extract()</span><br><span class="line"></span><br><span class="line">response.xpath(<span class="string">'//a[contains(@href,"image")]/img/@src'</span>).extract()</span><br><span class="line">response.css(<span class="string">'a[href*="imag"] img::attr(src)'</span>).extract()</span><br><span class="line"></span><br><span class="line">response.xpath(<span class="string">'//*[@href="image1.html"]'</span>)</span><br><span class="line">response.css(<span class="string">'*[href="image1.html"]'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#6、正则表达式</span></span><br><span class="line">response.xpath(<span class="string">'//a/text()'</span>).re(<span class="string">r'Name: (.*)'</span>)</span><br><span class="line">response.xpath(<span class="string">'//a/text()'</span>).re_first(<span class="string">r'Name: (.*)'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#7、xpath相对路径</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>res=response.xpath(<span class="string">'//a[contains(@href,"3")]'</span>)[<span class="number">0</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>res.xpath(<span class="string">'img'</span>)</span><br><span class="line">[&lt;Selector xpath=<span class="string">'img'</span> data=<span class="string">'&lt;img src="image3_thumb.jpg"&gt;'</span>&gt;]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>res.xpath(<span class="string">'./img'</span>)</span><br><span class="line">[&lt;Selector xpath=<span class="string">'./img'</span> data=<span class="string">'&lt;img src="image3_thumb.jpg"&gt;'</span>&gt;]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>res.xpath(<span class="string">'.//img'</span>)</span><br><span class="line">[&lt;Selector xpath=<span class="string">'.//img'</span> data=<span class="string">'&lt;img src="image3_thumb.jpg"&gt;'</span>&gt;]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>res.xpath(<span class="string">'//img'</span>) <span class="comment">#这就是从头开始扫描</span></span><br><span class="line">[&lt;Selector xpath=<span class="string">'//img'</span> data=<span class="string">'&lt;img src="image1_thumb.jpg"&gt;'</span>&gt;, &lt;Selector xpath=<span class="string">'//img'</span> data=<span class="string">'&lt;img src="image2_thumb.jpg"&gt;'</span>&gt;, &lt;Selector xpath=<span class="string">'//img'</span> data=<span class="string">'&lt;img src="image3_thumb.jpg"&gt;'</span>&gt;, &lt;Selector xpa</span><br><span class="line">th=<span class="string">'//img'</span> data=<span class="string">'&lt;img src="image4_thumb.jpg"&gt;'</span>&gt;, &lt;Selector xpath=<span class="string">'//img'</span> data=<span class="string">'&lt;img src="image5_thumb.jpg"&gt;'</span>&gt;]</span><br><span class="line"></span><br><span class="line"><span class="comment">#8、带变量的xpath</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.xpath(<span class="string">'//div[@id=$xxx]/a/text()'</span>,xxx=<span class="string">'images'</span>).extract_first()</span><br><span class="line"><span class="string">'Name: My image 1 '</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.xpath(<span class="string">'//div[count(a)=$yyy]/@id'</span>,yyy=<span class="number">5</span>).extract_first() <span class="comment">#求有5个a标签的div的id</span></span><br><span class="line"><span class="string">'images'</span></span><br></pre></td></tr></tbody></table></figure>

<p><a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/selectors.html">https://docs.scrapy.org/en/latest/topics/selectors.html</a></p>
<h3 id="七-Items"><a href="#七-Items" class="headerlink" title="七 Items"></a>七 Items</h3><p><a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/items.html">https://docs.scrapy.org/en/latest/topics/items.html</a></p>
<h3 id="八-Item-Pipeline"><a href="#八-Item-Pipeline" class="headerlink" title="八 Item Pipeline"></a>八 Item Pipeline</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#一：可以写多个Pipeline类</span></span><br><span class="line"><span class="comment">#1、如果优先级高的Pipeline的process_item返回一个值或者None，会自动传给下一个pipline的process_item,</span></span><br><span class="line"><span class="comment">#2、如果只想让第一个Pipeline执行，那得让第一个pipline的process_item抛出异常raise DropItem()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#3、可以用spider.name == '爬虫名' 来控制哪些爬虫用哪些pipeline</span></span><br><span class="line"></span><br><span class="line">二：示范</span><br><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomPipeline</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,v</span>):</span><br><span class="line">        self.value = v</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"><span class="meta">@classmethod</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">from_crawler</span>(<span class="params">cls, crawler</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Scrapy会先通过getattr判断我们是否自定义了from_crawler,有则调它来完</span></span><br><span class="line"><span class="string">    成实例化</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    val = crawler.settings.getint(<span class="string">'MMMM'</span>)</span><br><span class="line">    <span class="keyword">return</span> cls(val)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self,spider</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    爬虫刚启动时执行一次</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'000000'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self,spider</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    爬虫关闭时执行一次</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'111111'</span>)</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">    <span class="comment"># 操作并进行持久化</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># return表示会被后续的pipeline继续处理</span></span><br><span class="line">    <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 表示将item丢弃，不会被后续pipeline处理</span></span><br><span class="line">    <span class="comment"># raise DropItem()</span></span><br><span class="line"><span class="comment">#1、settings.py</span></span><br><span class="line">HOST=<span class="string">"127.0.0.1"</span></span><br><span class="line">PORT=<span class="number">27017</span></span><br><span class="line">USER=<span class="string">"root"</span></span><br><span class="line">PWD=<span class="string">"123"</span></span><br><span class="line">DB=<span class="string">"amazon"</span></span><br><span class="line">TABLE=<span class="string">"goods"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ITEM_PIPELINES = {</span><br><span class="line">   <span class="string">'Amazon.pipelines.CustomPipeline'</span>: <span class="number">200</span>,</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">#2、pipelines.py</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomPipeline</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,host,port,user,pwd,db,table</span>):</span><br><span class="line">        self.host=host</span><br><span class="line">        self.port=port</span><br><span class="line">        self.user=user</span><br><span class="line">        self.pwd=pwd</span><br><span class="line">        self.db=db</span><br><span class="line">        self.table=table</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"><span class="meta">@classmethod</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">from_crawler</span>(<span class="params">cls, crawler</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Scrapy会先通过getattr判断我们是否自定义了from_crawler,有则调它来完</span></span><br><span class="line"><span class="string">    成实例化</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    HOST = crawler.settings.get(<span class="string">'HOST'</span>)</span><br><span class="line">    PORT = crawler.settings.get(<span class="string">'PORT'</span>)</span><br><span class="line">    USER = crawler.settings.get(<span class="string">'USER'</span>)</span><br><span class="line">    PWD = crawler.settings.get(<span class="string">'PWD'</span>)</span><br><span class="line">    DB = crawler.settings.get(<span class="string">'DB'</span>)</span><br><span class="line">    TABLE = crawler.settings.get(<span class="string">'TABLE'</span>)</span><br><span class="line">    <span class="keyword">return</span> cls(HOST,PORT,USER,PWD,DB,TABLE)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self,spider</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    爬虫刚启动时执行一次</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    self.client = MongoClient(<span class="string">'mongodb://%s:%s@%s:%s'</span> %(self.user,self.pwd,self.host,self.port))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self,spider</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    爬虫关闭时执行一次</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    self.client.close()</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">    <span class="comment"># 操作并进行持久化</span></span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.client[self.db][self.table].save(<span class="built_in">dict</span>(item))</span><br></pre></td></tr></tbody></table></figure>

<p><a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/item-pipeline.html">https://docs.scrapy.org/en/latest/topics/item-pipeline.html</a></p>
<h3 id="九-Dowloader-Middeware"><a href="#九-Dowloader-Middeware" class="headerlink" title="九 Dowloader Middeware"></a>九 Dowloader Middeware</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line">下载中间件的用途</span><br><span class="line">    <span class="number">1</span>、在process——request内，自定义下载，不用scrapy的下载</span><br><span class="line">    <span class="number">2</span>、对请求进行二次加工，比如</span><br><span class="line">        设置请求头</span><br><span class="line">        设置cookie</span><br><span class="line">        添加代理</span><br><span class="line">            scrapy自带的代理组件：</span><br><span class="line">                <span class="keyword">from</span> scrapy.downloadermiddlewares.httpproxy <span class="keyword">import</span> HttpProxyMiddleware</span><br><span class="line">                <span class="keyword">from</span> urllib.request <span class="keyword">import</span> getproxies</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DownMiddleware1</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_request</span>(<span class="params">self, request, spider</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        请求需要被下载时，经过所有下载器中间件的process_request调用</span></span><br><span class="line"><span class="string">        :param request: </span></span><br><span class="line"><span class="string">        :param spider: </span></span><br><span class="line"><span class="string">        :return:  </span></span><br><span class="line"><span class="string">            None,继续后续中间件去下载；</span></span><br><span class="line"><span class="string">            Response对象，停止process_request的执行，开始执行process_response</span></span><br><span class="line"><span class="string">            Request对象，停止中间件的执行，将Request重新调度器</span></span><br><span class="line"><span class="string">            raise IgnoreRequest异常，停止process_request的执行，开始执行process_exception</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_response</span>(<span class="params">self, request, response, spider</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    spider处理完成，返回时调用</span></span><br><span class="line"><span class="string">    :param response:</span></span><br><span class="line"><span class="string">    :param result:</span></span><br><span class="line"><span class="string">    :param spider:</span></span><br><span class="line"><span class="string">    :return: </span></span><br><span class="line"><span class="string">        Response 对象：转交给其他中间件process_response</span></span><br><span class="line"><span class="string">        Request 对象：停止中间件，request会被重新调度下载</span></span><br><span class="line"><span class="string">        raise IgnoreRequest 异常：调用Request.errback</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'response1'</span>)</span><br><span class="line">    <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_exception</span>(<span class="params">self, request, exception, spider</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    当下载处理器(download handler)或 process_request() (下载中间件)抛出异常</span></span><br><span class="line"><span class="string">    :param response:</span></span><br><span class="line"><span class="string">    :param exception:</span></span><br><span class="line"><span class="string">    :param spider:</span></span><br><span class="line"><span class="string">    :return: </span></span><br><span class="line"><span class="string">        None：继续交给后续中间件处理异常；</span></span><br><span class="line"><span class="string">        Response对象：停止后续process_exception方法</span></span><br><span class="line"><span class="string">        Request对象：停止中间件，request将会被重新调用下载</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"><span class="comment">#1、与middlewares.py同级目录下新建proxy_handle.py</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_proxy</span>():</span><br><span class="line">    <span class="keyword">return</span> requests.get(<span class="string">"http://127.0.0.1:5010/get/"</span>).text</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">delete_proxy</span>(<span class="params">proxy</span>):</span><br><span class="line">    requests.get(<span class="string">"http://127.0.0.1:5010/delete/?proxy={}"</span>.<span class="built_in">format</span>(proxy))</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="comment">#2、middlewares.py</span></span><br><span class="line"><span class="keyword">from</span> Amazon.proxy_handle <span class="keyword">import</span> get_proxy,delete_proxy</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DownMiddleware1</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_request</span>(<span class="params">self, request, spider</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        请求需要被下载时，经过所有下载器中间件的process_request调用</span></span><br><span class="line"><span class="string">        :param request:</span></span><br><span class="line"><span class="string">        :param spider:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">            None,继续后续中间件去下载；</span></span><br><span class="line"><span class="string">            Response对象，停止process_request的执行，开始执行process_response</span></span><br><span class="line"><span class="string">            Request对象，停止中间件的执行，将Request重新调度器</span></span><br><span class="line"><span class="string">            raise IgnoreRequest异常，停止process_request的执行，开始执行process_exception</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        proxy=<span class="string">"http://"</span> + get_proxy()</span><br><span class="line">        request.meta[<span class="string">'download_timeout'</span>]=<span class="number">20</span></span><br><span class="line">        request.meta[<span class="string">"proxy"</span>] = proxy</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">'为%s 添加代理%s '</span> % (request.url, proxy),end=<span class="string">''</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">'元数据为'</span>,request.meta)</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_response</span>(<span class="params">self, request, response, spider</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    spider处理完成，返回时调用</span></span><br><span class="line"><span class="string">    :param response:</span></span><br><span class="line"><span class="string">    :param result:</span></span><br><span class="line"><span class="string">    :param spider:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">        Response 对象：转交给其他中间件process_response</span></span><br><span class="line"><span class="string">        Request 对象：停止中间件，request会被重新调度下载</span></span><br><span class="line"><span class="string">        raise IgnoreRequest 异常：调用Request.errback</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'返回状态吗'</span>,response.status)</span><br><span class="line">    <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_exception</span>(<span class="params">self, request, exception, spider</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    当下载处理器(download handler)或 process_request() (下载中间件)抛出异常</span></span><br><span class="line"><span class="string">    :param response:</span></span><br><span class="line"><span class="string">    :param exception:</span></span><br><span class="line"><span class="string">    :param spider:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">        None：继续交给后续中间件处理异常；</span></span><br><span class="line"><span class="string">        Response对象：停止后续process_exception方法</span></span><br><span class="line"><span class="string">        Request对象：停止中间件，request将会被重新调用下载</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'代理%s，访问%s出现异常:%s'</span> %(request.meta[<span class="string">'proxy'</span>],request.url,exception))</span><br><span class="line">    <span class="keyword">import</span> time</span><br><span class="line">    time.sleep(<span class="number">5</span>)</span><br><span class="line">    delete_proxy(request.meta[<span class="string">'proxy'</span>].split(<span class="string">"//"</span>)[-<span class="number">1</span>])</span><br><span class="line">    request.meta[<span class="string">'proxy'</span>]=<span class="string">'http://'</span>+get_proxy()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> request</span><br></pre></td></tr></tbody></table></figure>

<h3 id="十-Spider-Middleware"><a href="#十-Spider-Middleware" class="headerlink" title="十 Spider Middleware"></a>十 Spider Middleware</h3><p><strong>1、爬虫中间件方法介绍</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> signals</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SpiderMiddleware</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="comment"># Not all methods need to be defined. If a method is not defined,</span></span><br><span class="line">    <span class="comment"># scrapy acts as if the spider middleware does not modify the</span></span><br><span class="line">    <span class="comment"># passed objects.</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_crawler</span>(<span class="params">cls, crawler</span>):</span><br><span class="line">        <span class="comment"># This method is used by Scrapy to create your spiders.</span></span><br><span class="line">        s = cls()</span><br><span class="line">        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened) <span class="comment">#当前爬虫执行时触发spider_opened</span></span><br><span class="line">        <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">spider_opened</span>(<span class="params">self, spider</span>):</span><br><span class="line">    <span class="comment"># spider.logger.info('我是egon派来的爬虫1: %s' % spider.name)</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'我是egon派来的爬虫1: %s'</span> % spider.name)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_start_requests</span>(<span class="params">self, start_requests, spider</span>):</span><br><span class="line">    <span class="comment"># Called with the start requests of the spider, and works</span></span><br><span class="line">    <span class="comment"># similarly to the process_spider_output() method, except</span></span><br><span class="line">    <span class="comment"># that it doesn’t have a response associated.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Must return only requests (not items).</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'start_requests1'</span>)</span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> start_requests:</span><br><span class="line">        <span class="keyword">yield</span> r</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_spider_input</span>(<span class="params">self, response, spider</span>):</span><br><span class="line">    <span class="comment"># Called for each response that goes through the spider</span></span><br><span class="line">    <span class="comment"># middleware and into the spider.</span></span><br><span class="line">    <span class="comment"># 每个response经过爬虫中间件进入spider时调用</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回值：Should return None or raise an exception.</span></span><br><span class="line">    <span class="comment">#1、None: 继续执行其他中间件的process_spider_input</span></span><br><span class="line">    <span class="comment">#2、抛出异常：</span></span><br><span class="line">    <span class="comment"># 一旦抛出异常则不再执行其他中间件的process_spider_input</span></span><br><span class="line">    <span class="comment"># 并且触发request绑定的errback</span></span><br><span class="line">    <span class="comment"># errback的返回值倒着传给中间件的process_spider_output</span></span><br><span class="line">    <span class="comment"># 如果未找到errback，则倒着执行中间件的process_spider_exception</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"input1"</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_spider_output</span>(<span class="params">self, response, result, spider</span>):</span><br><span class="line">    <span class="comment"># Called with the results returned from the Spider, after</span></span><br><span class="line">    <span class="comment"># it has processed the response.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Must return an iterable of Request, dict or Item objects.</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'output1'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用yield返回多次，与return返回一次是一个道理</span></span><br><span class="line">    <span class="comment"># 如果生成器掌握不好（函数内有yield执行函数得到的是生成器而并不会立刻执行），生成器的形式会容易误导你对中间件执行顺序的理解</span></span><br><span class="line">    <span class="comment"># for i in result:</span></span><br><span class="line">    <span class="comment">#     yield i</span></span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_spider_exception</span>(<span class="params">self, response, exception, spider</span>):</span><br><span class="line">    <span class="comment"># Called when a spider or process_spider_input() method</span></span><br><span class="line">    <span class="comment"># (from other spider middleware) raises an exception.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Should return either None or an iterable of Response, dict</span></span><br><span class="line">    <span class="comment"># or Item objects.</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'exception1'</span>)</span><br></pre></td></tr></tbody></table></figure>

<p><strong>2、当前爬虫启动时以及初始请求产生时</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#步骤一：</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">打开注释：</span></span><br><span class="line"><span class="string">SPIDER_MIDDLEWARES = {</span></span><br><span class="line"><span class="string">   'Baidu.middlewares.SpiderMiddleware1': 200,</span></span><br><span class="line"><span class="string">   'Baidu.middlewares.SpiderMiddleware2': 300,</span></span><br><span class="line"><span class="string">   'Baidu.middlewares.SpiderMiddleware3': 400,</span></span><br><span class="line"><span class="string">}</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#步骤二：middlewares.py</span></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> signals</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SpiderMiddleware1</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_crawler</span>(<span class="params">cls, crawler</span>):</span><br><span class="line">        s = cls()</span><br><span class="line">        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened) <span class="comment">#当前爬虫执行时触发spider_opened</span></span><br><span class="line">        <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">spider_opened</span>(<span class="params">self, spider</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'我是egon派来的爬虫1: %s'</span> % spider.name)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_start_requests</span>(<span class="params">self, start_requests, spider</span>):</span><br><span class="line">    <span class="comment"># Must return only requests (not items).</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'start_requests1'</span>)</span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> start_requests:</span><br><span class="line">        <span class="keyword">yield</span> r</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SpiderMiddleware2</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_crawler</span>(<span class="params">cls, crawler</span>):</span><br><span class="line">        s = cls()</span><br><span class="line">        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)  <span class="comment"># 当前爬虫执行时触发spider_opened</span></span><br><span class="line">        <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">spider_opened</span>(<span class="params">self, spider</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'我是egon派来的爬虫2: %s'</span> % spider.name)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_start_requests</span>(<span class="params">self, start_requests, spider</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'start_requests2'</span>)</span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> start_requests:</span><br><span class="line">        <span class="keyword">yield</span> r</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SpiderMiddleware3</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_crawler</span>(<span class="params">cls, crawler</span>):</span><br><span class="line">        s = cls()</span><br><span class="line">        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)  <span class="comment"># 当前爬虫执行时触发spider_opened</span></span><br><span class="line">        <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">spider_opened</span>(<span class="params">self, spider</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'我是egon派来的爬虫3: %s'</span> % spider.name)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_start_requests</span>(<span class="params">self, start_requests, spider</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'start_requests3'</span>)</span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> start_requests:</span><br><span class="line">        <span class="keyword">yield</span> r</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"></span><br><span class="line"><span class="comment">#步骤三：分析运行结果</span></span><br><span class="line"><span class="comment">#1、启动爬虫时则立刻执行：</span></span><br><span class="line"></span><br><span class="line">我是egon派来的爬虫<span class="number">1</span>: baidu</span><br><span class="line">我是egon派来的爬虫<span class="number">2</span>: baidu</span><br><span class="line">我是egon派来的爬虫<span class="number">3</span>: baidu</span><br><span class="line"></span><br><span class="line"><span class="comment">#2、然后产生一个初始的request请求，依次经过爬虫中间件1,2,3：</span></span><br><span class="line">start_requests1</span><br><span class="line">start_requests2</span><br><span class="line">start_requests3</span><br></pre></td></tr></tbody></table></figure>

<p><strong>3、process_spider_input返回None时</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#步骤一：打开注释：</span></span><br><span class="line">SPIDER_MIDDLEWARES = {</span><br><span class="line">   <span class="string">'Baidu.middlewares.SpiderMiddleware1'</span>: <span class="number">200</span>,</span><br><span class="line">   <span class="string">'Baidu.middlewares.SpiderMiddleware2'</span>: <span class="number">300</span>,</span><br><span class="line">   <span class="string">'Baidu.middlewares.SpiderMiddleware3'</span>: <span class="number">400</span>,</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#步骤二：middlewares.py</span></span><br><span class="line"><span class="string">from scrapy import signals</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">class SpiderMiddleware1(object):</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">```</span></span><br><span class="line"><span class="string">def process_spider_input(self, response, spider):</span></span><br><span class="line"><span class="string">    print("input1")</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">def process_spider_output(self, response, result, spider):</span></span><br><span class="line"><span class="string">    print('output1')</span></span><br><span class="line"><span class="string">    return result</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">def process_spider_exception(self, response, exception, spider):</span></span><br><span class="line"><span class="string">    print('exception1')</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">```</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">class SpiderMiddleware2(object):</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">```</span></span><br><span class="line"><span class="string">def process_spider_input(self, response, spider):</span></span><br><span class="line"><span class="string">    print("input2")</span></span><br><span class="line"><span class="string">    return None</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">def process_spider_output(self, response, result, spider):</span></span><br><span class="line"><span class="string">    print('output2')</span></span><br><span class="line"><span class="string">    return result</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">def process_spider_exception(self, response, exception, spider):</span></span><br><span class="line"><span class="string">    print('exception2')</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">```</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">class SpiderMiddleware3(object):</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">```</span></span><br><span class="line"><span class="string">def process_spider_input(self, response, spider):</span></span><br><span class="line"><span class="string">    print("input3")</span></span><br><span class="line"><span class="string">    return None</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">def process_spider_output(self, response, result, spider):</span></span><br><span class="line"><span class="string">    print('output3')</span></span><br><span class="line"><span class="string">    return result</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">def process_spider_exception(self, response, exception, spider):</span></span><br><span class="line"><span class="string">    print('exception3')</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">```</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#步骤三：运行结果分析</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#1、返回response时，依次经过爬虫中间件1,2,3</span></span><br><span class="line"><span class="string">input1</span></span><br><span class="line"><span class="string">input2</span></span><br><span class="line"><span class="string">input3</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#2、spider处理完毕后，依次经过爬虫中间件3,2,1</span></span><br><span class="line"><span class="string">output3</span></span><br><span class="line"><span class="string">output2</span></span><br><span class="line"><span class="string">output1</span></span><br></pre></td></tr></tbody></table></figure>

<p><strong>4、process_spider_input抛出异常时</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#步骤一：</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">打开注释：</span></span><br><span class="line"><span class="string">SPIDER_MIDDLEWARES = {</span></span><br><span class="line"><span class="string">   'Baidu.middlewares.SpiderMiddleware1': 200,</span></span><br><span class="line"><span class="string">   'Baidu.middlewares.SpiderMiddleware2': 300,</span></span><br><span class="line"><span class="string">   'Baidu.middlewares.SpiderMiddleware3': 400,</span></span><br><span class="line"><span class="string">}</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#步骤二：middlewares.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> signals</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SpiderMiddleware1</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_spider_input</span>(<span class="params">self, response, spider</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"input1"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_spider_output</span>(<span class="params">self, response, result, spider</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'output1'</span>)</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_spider_exception</span>(<span class="params">self, response, exception, spider</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'exception1'</span>)</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SpiderMiddleware2</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_spider_input</span>(<span class="params">self, response, spider</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"input2"</span>)</span><br><span class="line">    <span class="keyword">raise</span> <span class="type">Type</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_spider_output</span>(<span class="params">self, response, result, spider</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'output2'</span>)</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_spider_exception</span>(<span class="params">self, response, exception, spider</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'exception2'</span>)</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SpiderMiddleware3</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_spider_input</span>(<span class="params">self, response, spider</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"input3"</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_spider_output</span>(<span class="params">self, response, result, spider</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'output3'</span>)</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_spider_exception</span>(<span class="params">self, response, exception, spider</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'exception3'</span>)</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="comment">#运行结果        </span></span><br><span class="line">input1</span><br><span class="line">input2</span><br><span class="line">exception3</span><br><span class="line">exception2</span><br><span class="line">exception1</span><br><span class="line"></span><br><span class="line"><span class="comment">#分析：</span></span><br><span class="line"><span class="comment">#1、当response经过中间件1的 process_spider_input返回None，继续交给中间件2的process_spider_input</span></span><br><span class="line"><span class="comment">#2、中间件2的process_spider_input抛出异常，则直接跳过后续的process_spider_input，将异常信息传递给Spiders里该请求的errback</span></span><br><span class="line"><span class="comment">#3、没有找到errback，则该response既没有被Spiders正常的callback执行，也没有被errback执行，即Spiders啥事也没有干，那么开始倒着执行process_spider_exception</span></span><br><span class="line"><span class="comment">#4、如果process_spider_exception返回None，代表该方法推卸掉责任，并没处理异常，而是直接交给下一个process_spider_exception，全都返回None，则异常最终交给Engine抛出</span></span><br></pre></td></tr></tbody></table></figure>

<p><strong>5、指定errback</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#步骤一：spider.py</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BaiduSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">'baidu'</span></span><br><span class="line">    allowed_domains = [<span class="string">'www.baidu.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://www.baidu.com/'</span>]</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">start_requests</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="keyword">yield</span> scrapy.Request(url=<span class="string">'http://www.baidu.com/'</span>,</span><br><span class="line">                         callback=self.parse,</span><br><span class="line">                         errback=self.parse_err,</span><br><span class="line">                         )</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse_err</span>(<span class="params">self,res</span>):</span><br><span class="line">    <span class="comment">#res 为异常信息，异常已经被该函数处理了，因此不会再抛给因此，于是开始走process_spider_output</span></span><br><span class="line">    <span class="keyword">return</span> [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>] <span class="comment">#提取异常信息中有用的数据以可迭代对象的形式存放于管道中，等待被process_spider_output取走</span></span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#步骤二：</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">打开注释：</span></span><br><span class="line"><span class="string">SPIDER_MIDDLEWARES = {</span></span><br><span class="line"><span class="string">   'Baidu.middlewares.SpiderMiddleware1': 200,</span></span><br><span class="line"><span class="string">   'Baidu.middlewares.SpiderMiddleware2': 300,</span></span><br><span class="line"><span class="string">   'Baidu.middlewares.SpiderMiddleware3': 400,</span></span><br><span class="line"><span class="string">}</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#步骤三：middlewares.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> signals</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SpiderMiddleware1</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_spider_input</span>(<span class="params">self, response, spider</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"input1"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_spider_output</span>(<span class="params">self, response, result, spider</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'output1'</span>,<span class="built_in">list</span>(result))</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_spider_exception</span>(<span class="params">self, response, exception, spider</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'exception1'</span>)</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SpiderMiddleware2</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_spider_input</span>(<span class="params">self, response, spider</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"input2"</span>)</span><br><span class="line">    <span class="keyword">raise</span> TypeError(<span class="string">'input2 抛出异常'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_spider_output</span>(<span class="params">self, response, result, spider</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'output2'</span>,<span class="built_in">list</span>(result))</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_spider_exception</span>(<span class="params">self, response, exception, spider</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'exception2'</span>)</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SpiderMiddleware3</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_spider_input</span>(<span class="params">self, response, spider</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"input3"</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_spider_output</span>(<span class="params">self, response, result, spider</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'output3'</span>,<span class="built_in">list</span>(result))</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_spider_exception</span>(<span class="params">self, response, exception, spider</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'exception3'</span>)</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#步骤四：运行结果分析</span></span><br><span class="line">input1</span><br><span class="line">input2</span><br><span class="line">output3 [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>] <span class="comment">#parse_err的返回值放入管道中，只能被取走一次，在output3的方法内可以根据异常信息封装一个新的request请求</span></span><br><span class="line">output2 []</span><br><span class="line">output1 []</span><br></pre></td></tr></tbody></table></figure>

<h3 id="十一-自定义扩展"><a href="#十一-自定义扩展" class="headerlink" title="十一 自定义扩展"></a>十一 自定义扩展</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">自定义扩展（与django的信号类似）</span><br><span class="line">    <span class="number">1</span>、django的信号是django是预留的扩展，信号一旦被触发，相应的功能就会执行</span><br><span class="line">    <span class="number">2</span>、scrapy自定义扩展的好处是可以在任意我们想要的位置添加功能，而其他组件中提供的功能只能在规定的位置执行</span><br><span class="line"><span class="comment">#1、在与settings同级目录下新建一个文件，文件名可以为extentions.py,内容如下</span></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> signals</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyExtension</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, value</span>):</span><br><span class="line">        self.value = value</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"><span class="meta">@classmethod</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">from_crawler</span>(<span class="params">cls, crawler</span>):</span><br><span class="line">    val = crawler.settings.getint(<span class="string">'MMMM'</span>)</span><br><span class="line">    obj = cls(val)</span><br><span class="line"></span><br><span class="line">    crawler.signals.connect(obj.spider_opened, signal=signals.spider_opened)</span><br><span class="line">    crawler.signals.connect(obj.spider_closed, signal=signals.spider_closed)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> obj</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">spider_opened</span>(<span class="params">self, spider</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'=============&gt;open'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">spider_closed</span>(<span class="params">self, spider</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'=============&gt;close'</span>)</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"></span><br><span class="line"><span class="comment">#2、配置生效</span></span><br><span class="line">EXTENSIONS = {</span><br><span class="line">    <span class="string">"Amazon.extentions.MyExtension"</span>:<span class="number">200</span></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<h3 id="十二-settings-py"><a href="#十二-settings-py" class="headerlink" title="十二 settings.py"></a>十二 settings.py</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#==&gt;第一部分：基本配置&lt;===</span></span><br><span class="line"><span class="comment">#1、项目名称，默认的USER_AGENT由它来构成，也作为日志记录的日志名</span></span><br><span class="line">BOT_NAME = <span class="string">'Amazon'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2、爬虫应用路径</span></span><br><span class="line">SPIDER_MODULES = [<span class="string">'Amazon.spiders'</span>]</span><br><span class="line">NEWSPIDER_MODULE = <span class="string">'Amazon.spiders'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#3、客户端User-Agent请求头</span></span><br><span class="line"><span class="comment">#USER_AGENT = 'Amazon (+http://www.yourdomain.com)'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#4、是否遵循爬虫协议</span></span><br><span class="line"><span class="comment"># Obey robots.txt rules</span></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#5、是否支持cookie，cookiejar进行操作cookie，默认开启</span></span><br><span class="line"><span class="comment">#COOKIES_ENABLED = False</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#6、Telnet用于查看当前爬虫的信息，操作爬虫等...使用telnet ip port ，然后通过命令操作</span></span><br><span class="line"><span class="comment">#TELNETCONSOLE_ENABLED = False</span></span><br><span class="line"><span class="comment">#TELNETCONSOLE_HOST = '127.0.0.1'</span></span><br><span class="line"><span class="comment">#TELNETCONSOLE_PORT = [6023,]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#7、Scrapy发送HTTP请求默认使用的请求头</span></span><br><span class="line"><span class="comment">#DEFAULT_REQUEST_HEADERS = {</span></span><br><span class="line"><span class="comment">#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',</span></span><br><span class="line"><span class="comment">#   'Accept-Language': 'en',</span></span><br><span class="line"><span class="comment">#}</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#===&gt;第二部分：并发与延迟&lt;===</span></span><br><span class="line"><span class="comment">#1、下载器总共最大处理的并发请求数,默认值16</span></span><br><span class="line"><span class="comment">#CONCURRENT_REQUESTS = 32</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2、每个域名能够被执行的最大并发请求数目，默认值8</span></span><br><span class="line"><span class="comment">#CONCURRENT_REQUESTS_PER_DOMAIN = 16</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#3、能够被单个IP处理的并发请求数，默认值0，代表无限制，需要注意两点</span></span><br><span class="line"><span class="comment">#I、如果不为零，那CONCURRENT_REQUESTS_PER_DOMAIN将被忽略，即并发数的限制是按照每个IP来计算，而不是每个域名</span></span><br><span class="line"><span class="comment">#II、该设置也影响DOWNLOAD_DELAY，如果该值不为零，那么DOWNLOAD_DELAY下载延迟是限制每个IP而不是每个域</span></span><br><span class="line"><span class="comment">#CONCURRENT_REQUESTS_PER_IP = 16</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#4、如果没有开启智能限速，这个值就代表一个规定死的值，代表对同一网址延迟请求的秒数</span></span><br><span class="line"><span class="comment">#DOWNLOAD_DELAY = 3</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#===&gt;第三部分：智能限速/自动节流：AutoThrottle extension&lt;===</span></span><br><span class="line"><span class="comment">#一：介绍</span></span><br><span class="line"><span class="keyword">from</span> scrapy.contrib.throttle <span class="keyword">import</span> AutoThrottle <span class="comment">#http://scrapy.readthedocs.io/en/latest/topics/autothrottle.html#topics-autothrottle</span></span><br><span class="line">设置目标：</span><br><span class="line"><span class="number">1</span>、比使用默认的下载延迟对站点更好</span><br><span class="line"><span class="number">2</span>、自动调整scrapy到最佳的爬取速度，所以用户无需自己调整下载延迟到最佳状态。用户只需要定义允许最大并发的请求，剩下的事情由该扩展组件自动完成</span><br><span class="line"></span><br><span class="line"><span class="comment">#二：如何实现？</span></span><br><span class="line">在Scrapy中，下载延迟是通过计算建立TCP连接到接收到HTTP包头(header)之间的时间来测量的。</span><br><span class="line">注意，由于Scrapy可能在忙着处理spider的回调函数或者无法下载，因此在合作的多任务环境下准确测量这些延迟是十分苦难的。 不过，这些延迟仍然是对Scrapy(甚至是服务器)繁忙程度的合理测量，而这扩展就是以此为前提进行编写的。</span><br><span class="line"></span><br><span class="line"><span class="comment">#三：限速算法</span></span><br><span class="line">自动限速算法基于以下规则调整下载延迟</span><br><span class="line"><span class="comment">#1、spiders开始时的下载延迟是基于AUTOTHROTTLE_START_DELAY的值</span></span><br><span class="line"><span class="comment">#2、当收到一个response，对目标站点的下载延迟=收到响应的延迟时间/AUTOTHROTTLE_TARGET_CONCURRENCY</span></span><br><span class="line"><span class="comment">#3、下一次请求的下载延迟就被设置成：对目标站点下载延迟时间和过去的下载延迟时间的平均值</span></span><br><span class="line"><span class="comment">#4、没有达到200个response则不允许降低延迟</span></span><br><span class="line"><span class="comment">#5、下载延迟不能变的比DOWNLOAD_DELAY更低或者比AUTOTHROTTLE_MAX_DELAY更高</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#四：配置使用</span></span><br><span class="line"><span class="comment">#开启True，默认False</span></span><br><span class="line">AUTOTHROTTLE_ENABLED = <span class="literal">True</span></span><br><span class="line"><span class="comment">#起始的延迟</span></span><br><span class="line">AUTOTHROTTLE_START_DELAY = <span class="number">5</span></span><br><span class="line"><span class="comment">#最小延迟</span></span><br><span class="line">DOWNLOAD_DELAY = <span class="number">3</span></span><br><span class="line"><span class="comment">#最大延迟</span></span><br><span class="line">AUTOTHROTTLE_MAX_DELAY = <span class="number">10</span></span><br><span class="line"><span class="comment">#每秒并发请求数的平均值，不能高于 CONCURRENT_REQUESTS_PER_DOMAIN或CONCURRENT_REQUESTS_PER_IP，调高了则吞吐量增大强奸目标站点，调低了则对目标站点更加”礼貌“</span></span><br><span class="line"><span class="comment">#每个特定的时间点，scrapy并发请求的数目都可能高于或低于该值，这是爬虫视图达到的建议值而不是硬限制</span></span><br><span class="line">AUTOTHROTTLE_TARGET_CONCURRENCY = <span class="number">16.0</span></span><br><span class="line"><span class="comment">#调试</span></span><br><span class="line">AUTOTHROTTLE_DEBUG = <span class="literal">True</span></span><br><span class="line">CONCURRENT_REQUESTS_PER_DOMAIN = <span class="number">16</span></span><br><span class="line">CONCURRENT_REQUESTS_PER_IP = <span class="number">16</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#===&gt;第四部分：爬取深度与爬取方式&lt;===</span></span><br><span class="line"><span class="comment">#1、爬虫允许的最大深度，可以通过meta查看当前深度；0表示无深度</span></span><br><span class="line"><span class="comment"># DEPTH_LIMIT = 3</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2、爬取时，0表示深度优先Lifo(默认)；1表示广度优先FiFo</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 后进先出，深度优先</span></span><br><span class="line"><span class="comment"># DEPTH_PRIORITY = 0</span></span><br><span class="line"><span class="comment"># SCHEDULER_DISK_QUEUE = 'scrapy.squeue.PickleLifoDiskQueue'</span></span><br><span class="line"><span class="comment"># SCHEDULER_MEMORY_QUEUE = 'scrapy.squeue.LifoMemoryQueue'</span></span><br><span class="line"><span class="comment"># 先进先出，广度优先</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DEPTH_PRIORITY = 1</span></span><br><span class="line"><span class="comment"># SCHEDULER_DISK_QUEUE = 'scrapy.squeue.PickleFifoDiskQueue'</span></span><br><span class="line"><span class="comment"># SCHEDULER_MEMORY_QUEUE = 'scrapy.squeue.FifoMemoryQueue'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#3、调度器队列</span></span><br><span class="line"><span class="comment"># SCHEDULER = 'scrapy.core.scheduler.Scheduler'</span></span><br><span class="line"><span class="comment"># from scrapy.core.scheduler import Scheduler</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#4、访问URL去重</span></span><br><span class="line"><span class="comment"># DUPEFILTER_CLASS = 'step8_king.duplication.RepeatUrl'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#===&gt;第五部分：中间件、Pipelines、扩展&lt;===</span></span><br><span class="line"><span class="comment">#1、Enable or disable spider middlewares</span></span><br><span class="line"><span class="comment"># See http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html</span></span><br><span class="line"><span class="comment">#SPIDER_MIDDLEWARES = {</span></span><br><span class="line"><span class="comment">#    'Amazon.middlewares.AmazonSpiderMiddleware': 543,</span></span><br><span class="line"><span class="comment">#}</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2、Enable or disable downloader middlewares</span></span><br><span class="line"><span class="comment"># See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html</span></span><br><span class="line">DOWNLOADER_MIDDLEWARES = {</span><br><span class="line">   <span class="comment"># 'Amazon.middlewares.DownMiddleware1': 543,</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">#3、Enable or disable extensions</span></span><br><span class="line"><span class="comment"># See http://scrapy.readthedocs.org/en/latest/topics/extensions.html</span></span><br><span class="line"><span class="comment">#EXTENSIONS = {</span></span><br><span class="line"><span class="comment">#    'scrapy.extensions.telnet.TelnetConsole': None,</span></span><br><span class="line"><span class="comment">#}</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#4、Configure item pipelines</span></span><br><span class="line"><span class="comment"># See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line">ITEM_PIPELINES = {</span><br><span class="line">   <span class="comment"># 'Amazon.pipelines.CustomPipeline': 200,</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#===&gt;第六部分：缓存&lt;===</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">1. 启用缓存</span></span><br><span class="line"><span class="string">   目的用于将已经发送的请求或相应缓存下来，以便以后使用</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">   from scrapy.downloadermiddlewares.httpcache import HttpCacheMiddleware</span></span><br><span class="line"><span class="string">   from scrapy.extensions.httpcache import DummyPolicy</span></span><br><span class="line"><span class="string">   from scrapy.extensions.httpcache import FilesystemCacheStorage</span></span><br><span class="line"><span class="string">   """</span></span><br><span class="line">   <span class="comment"># 是否启用缓存策略</span></span><br><span class="line">   <span class="comment"># HTTPCACHE_ENABLED = True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 缓存策略：所有请求均缓存，下次在请求直接访问原来的缓存即可</span></span><br><span class="line"><span class="comment"># HTTPCACHE_POLICY = "scrapy.extensions.httpcache.DummyPolicy"</span></span><br><span class="line"><span class="comment"># 缓存策略：根据Http响应头：Cache-Control、Last-Modified 等进行缓存的策略</span></span><br><span class="line"><span class="comment"># HTTPCACHE_POLICY = "scrapy.extensions.httpcache.RFC2616Policy"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 缓存超时时间</span></span><br><span class="line"><span class="comment"># HTTPCACHE_EXPIRATION_SECS = 0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 缓存保存路径</span></span><br><span class="line"><span class="comment"># HTTPCACHE_DIR = 'httpcache'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 缓存忽略的Http状态码</span></span><br><span class="line"><span class="comment"># HTTPCACHE_IGNORE_HTTP_CODES = []</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 缓存存储的插件</span></span><br><span class="line"><span class="comment"># HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#===&gt;第七部分：线程池&lt;===</span></span><br><span class="line">REACTOR_THREADPOOL_MAXSIZE = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Default: 10</span></span><br><span class="line"><span class="comment">#scrapy基于twisted异步IO框架，downloader是多线程的，线程数是Twisted线程池的默认大小(The maximum limit for Twisted Reactor thread pool size.)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#关于twisted线程池：</span></span><br><span class="line">http://twistedmatrix.com/documents/<span class="number">10.1</span><span class="number">.0</span>/core/howto/threading.html</span><br><span class="line"></span><br><span class="line"><span class="comment">#线程池实现：twisted.python.threadpool.ThreadPool</span></span><br><span class="line">twisted调整线程池大小：</span><br><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> reactor</span><br><span class="line">reactor.suggestThreadPoolSize(<span class="number">30</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#scrapy相关源码：</span></span><br><span class="line">D:\python3<span class="number">.6</span>\Lib\site-packages\scrapy\crawler.py</span><br><span class="line"></span><br><span class="line"><span class="comment">#补充：</span></span><br><span class="line">windows下查看进程内线程数的工具：</span><br><span class="line">    https://docs.microsoft.com/zh-cn/sysinternals/downloads/pslist</span><br><span class="line">    或</span><br><span class="line">    https://pan.baidu.com/s/1jJ0pMaM</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">```</span><br><span class="line">命令为：</span><br><span class="line">pslist |findstr python</span><br><span class="line"></span><br><span class="line">```</span><br><span class="line"></span><br><span class="line">linux下：top -p 进程<span class="built_in">id</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#===&gt;第八部分：其他默认配置参考&lt;===</span></span><br><span class="line">D:\python3<span class="number">.6</span>\Lib\site-packages\scrapy\settings\default_settings.py</span><br></pre></td></tr></tbody></table></figure>

<h3 id="十三-自定制命令"><a href="#十三-自定制命令" class="headerlink" title="十三 自定制命令"></a>十三 自定制命令</h3><ul>
<li><p>在spiders同级创建任意目录，如：commands</p>
</li>
<li><p>在其中创建 crawlall.py 文件 （此处文件名就是自定义的命令）</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.commands <span class="keyword">import</span> ScrapyCommand</span><br><span class="line">  <span class="keyword">from</span> scrapy.utils.project <span class="keyword">import</span> get_project_settings</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  <span class="keyword">class</span> <span class="title class_">Command</span>(<span class="title class_ inherited__">ScrapyCommand</span>):</span><br><span class="line">  </span><br><span class="line">      requires_project = <span class="literal">True</span></span><br><span class="line">  </span><br><span class="line">      <span class="keyword">def</span> <span class="title function_">syntax</span>(<span class="params">self</span>):</span><br><span class="line">          <span class="keyword">return</span> <span class="string">'[options]'</span></span><br><span class="line">  </span><br><span class="line">      <span class="keyword">def</span> <span class="title function_">short_desc</span>(<span class="params">self</span>):</span><br><span class="line">          <span class="keyword">return</span> <span class="string">'Runs all of the spiders'</span></span><br><span class="line">  </span><br><span class="line">      <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self, args, opts</span>):</span><br><span class="line">          spider_list = self.crawler_process.spiders.<span class="built_in">list</span>()</span><br><span class="line">          <span class="keyword">for</span> name <span class="keyword">in</span> spider_list:</span><br><span class="line">              self.crawler_process.crawl(name, **opts.__dict__)</span><br><span class="line">          self.crawler_process.start()</span><br></pre></td></tr></tbody></table></figure>
</li>
<li><p>在settings.py 中添加配置 COMMANDS_MODULE = ‘项目名称.目录名称’</p>
</li>
<li><p>在项目目录执行命令：scrapy crawlall</p>
</li>
</ul>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">祈安</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="http://example.com/2021/10/09/Scrapy/">http://example.com/2021/10/09/Scrapy/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">祈安</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/%E7%88%AC%E8%99%AB-Scrapy/">
                                    <span class="chip bg-color">爬虫-Scrapy</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    
        <link rel="stylesheet" href="/libs/gitalk/gitalk.css">
<link rel="stylesheet" href="/css/my-gitalk.css">

<div class="card gitalk-card" data-aos="fade-up">
    <div class="comment_headling" style="font-size: 20px; font-weight: 700; position: relative; padding-left: 20px; top: 15px; padding-bottom: 5px;">
        <i class="fas fa-comments fa-fw" aria-hidden="true"></i>
        <span>评论</span>
    </div>
    <div id="gitalk-container" class="card-content"></div>
</div>

<script src="/libs/gitalk/gitalk.min.js"></script>
<script>
    let gitalk = new Gitalk({
        clientID: '',
        clientSecret: '',
        repo: '',
        owner: '',
        admin: null,
        id: '2021-10-09T19-23-21',
        distractionFreeMode: false  // Facebook-like distraction free mode
    });

    gitalk.render('gitalk-container');
</script>

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2021/10/10/Beautifulsoup4/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/2.jpg" class="responsive-img" alt="爬虫-Beautifulsoup4">
                        
                        <span class="card-title">爬虫-Beautifulsoup4</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2021-10-10
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/Beautifulsoup4/" class="post-category">
                                    Beautifulsoup4
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/%E7%88%AC%E8%99%AB-Beautifulsoup4/">
                        <span class="chip bg-color">爬虫-Beautifulsoup4</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2021/10/08/SQLAlchemy/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/11.jpg" class="responsive-img" alt="FLask-SQLAlchemy">
                        
                        <span class="card-title">FLask-SQLAlchemy</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2021-10-08
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/Flask/" class="post-category">
                                    Flask
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/FLask-SQLAlchemy/">
                        <span class="chip bg-color">FLask-SQLAlchemy</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('100')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE') {
            newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        }

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + '来源: 且听风吟<br />'
            + '文章作者: 祈安<br />'
            + '文章链接: <a href="' + url + '">' + url + '</a><br />'
            + '本文章著作权归作者所有，任何形式的转载都请注明出处。';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () {bodyElement.removeChild(newdiv);}, 200);
    });
</script>


<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="tencent"
                   type="playlist"
                   id="4628814494"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>

    
    <div class="container row center-align" style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2020-2022</span>
            
            <span id="year">2020</span>
            <a href="/about" target="_blank">祈安</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                class="white-color">81.5k</span>&nbsp;字
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:jiangychao@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=913126194" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 913126194" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

	
    

    

    
    <script type="text/javascript" src="/libs/background/ribbon-dynamic.js" async="async"></script>
    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/z16.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true,"react":{"opacity":0.7}}});</script></body>

</html>
